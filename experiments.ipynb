{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "\n",
    "from itertools import count\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic loop of gymnasium - how to run  environment \n",
    "env = gym.make(\"Ant-v5\", render_mode=\"human\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "        Circular buffer for interactions with environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, item):\n",
    "        if len(self.buffer) > self._next_idx:\n",
    "            self.buffer[self._next_idx] = item\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "        if self._next_idx == self.buffer_size - 1:\n",
    "            self._next_idx = 0\n",
    "        else:\n",
    "            self._next_idx = self._next_idx + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = [random.randint(0, len(self.buffer) - 1) for _ in range(batch_size)]\n",
    "        states   = [self.buffer[i][0] for i in indices]\n",
    "        actions  = [self.buffer[i][1] for i in indices]\n",
    "        rewards  = [self.buffer[i][2] for i in indices]\n",
    "        n_states = [self.buffer[i][3] for i in indices]\n",
    "        dones    = [self.buffer[i][4] for i in indices]\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "memory = ReplayMemory(buffer_size=50000) # default length of episode is 50, so it should fit around 50 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        It takes observation-action pair and outputs Q-value for it.\n",
    "    \"\"\"\n",
    "    def __init__(self, actions_space_size: int = 8, observation_stapce_size:int = 105, hidden_size: int=256):\n",
    "        # defaults set for MuJuCo ant\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.action_space_size = actions_space_size\n",
    "        self.observation_space_size = observation_stapce_size\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.observation_space_size + self.action_space_size, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "        )\n",
    "        # we don't apply activation as we want Q value as an ouput\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should be observations and actions concatenated into one vector x\n",
    "        q = self.net(x)\n",
    "        return q\n",
    "    \n",
    "    \n",
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "        It takes state as an input and outputs the best action\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space_size: int = 8, observation_stapce_size:int = 105, hidden_size: int=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space_size = action_space_size\n",
    "        self.observation_space_size = observation_stapce_size\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.observation_space_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.action_space_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # apparently Tanh works better as activation function in agent networks\n",
    "\n",
    "    def forward(self, s):\n",
    "        a = self.net(s)\n",
    "        return a\n",
    "\n",
    "q_origin_model = QNet().to(device)  # Q_phi\n",
    "q_target_model = QNet().to(device)  # Q_phi'\n",
    "_ = q_target_model.requires_grad_(False)  # target model doen't need grad\n",
    "\n",
    "mu_origin_model = PolicyNet().to(device)  # mu_theta\n",
    "mu_target_model = PolicyNet().to(device)  # mu_theta'\n",
    "_ = mu_target_model.requires_grad_(False)  # target model doen't need grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "opt_q = torch.optim.AdamW(q_origin_model.parameters(), lr=0.0005)\n",
    "opt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "def optimize(states, actions, rewards, next_states, dones):\n",
    "    # Convert to tensor\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).to(device).unsqueeze(1)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).to(device).unsqueeze(1)\n",
    "\n",
    "    criterion = nn.MSELoss(reduction=\"sum\").to(device)\n",
    "    # Optimize critic loss\n",
    "    opt_q.zero_grad()\n",
    "    q_org = q_origin_model(torch.concat((states, actions), dim=1))\n",
    "    mu_tgt_next = mu_target_model(next_states)\n",
    "    q_tgt_next = q_target_model(torch.concat((next_states, mu_tgt_next), dim=1))\n",
    "    q_tgt = rewards + gamma * (1.0 - dones) * q_tgt_next\n",
    "    loss_q = criterion(q_org, q_tgt)\n",
    "    loss_q.backward()\n",
    "    opt_q.step()\n",
    "\n",
    "    # element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "    # Optimize actor loss\n",
    "    opt_mu.zero_grad()\n",
    "    mu_org = mu_origin_model(states)\n",
    "    for p in q_origin_model.parameters():\n",
    "        p.requires_grad = False  # disable grad in q_origin_model before computation\n",
    "    q_tgt_max = q_origin_model(torch.concat((states, mu_org), dim=1))\n",
    "    (-q_tgt_max).sum().backward()\n",
    "    opt_mu.step()\n",
    "    for p in q_origin_model.parameters():\n",
    "        p.requires_grad = True  # enable grad again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.002\n",
    "\n",
    "def update_target():\n",
    "    for var, var_target in zip(q_origin_model.parameters(), q_target_model.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
    "    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ornstein-Uhlenbeck noise implemented by OpenAI\n",
    "Copied from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "\"\"\"\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "ou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=np.ones(1) * 0.05)\n",
    "\n",
    "# pick up action with Ornstein-Uhlenbeck noise\n",
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        s = np.array(s)\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        action_det = mu_origin_model(s_batch)\n",
    "        action_det = action_det.squeeze(dim=0)\n",
    "        noise = ou_action_noise()\n",
    "        action = action_det.cpu().numpy() + noise\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run episode0 with rewards 525.954129361965\n",
      "Run episode1 with rewards -446.2996623604365\n",
      "Run episode2 with rewards -2517.3954883102388\n",
      "Run episode3 with rewards -2316.6800278092087\n",
      "Run episode4 with rewards -1948.1135278185318\n",
      "Run episode5 with rewards -1897.7847039452236\n",
      "Run episode6 with rewards -1180.3051388115314\n",
      "Run episode7 with rewards -1799.9048270918413\n",
      "Run episode8 with rewards -1692.5315850866998\n",
      "Run episode9 with rewards -1086.784542977137\n",
      "Run episode10 with rewards -1352.932412805691\n",
      "Run episode11 with rewards -2288.8189931916827\n",
      "Run episode12 with rewards -2398.3149892193337\n",
      "Run episode13 with rewards -2281.5509581263764\n",
      "Run episode14 with rewards -2033.883330878749\n",
      "Run episode15 with rewards -2048.630915396296\n",
      "Run episode16 with rewards -2768.35453971072\n",
      "Run episode17 with rewards -2357.669173395878\n",
      "Run episode18 with rewards -2460.0282710922847\n",
      "Run episode19 with rewards -2726.073374791215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m     18\u001b[0m     states, actions, rewards, n_states, dones \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     update_target()\n\u001b[1;32m     21\u001b[0m s \u001b[38;5;241m=\u001b[39m state_next\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Optimize critic loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m opt_q\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m q_org \u001b[38;5;241m=\u001b[39m \u001b[43mq_origin_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m mu_tgt_next \u001b[38;5;241m=\u001b[39m mu_target_model(next_states)\n\u001b[1;32m     19\u001b[0m q_tgt_next \u001b[38;5;241m=\u001b[39m q_target_model(torch\u001b[38;5;241m.\u001b[39mconcat((next_states, mu_tgt_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mQNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# x should be observations and actions concatenated into one vector x\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "reward_records = []\n",
    "for i in range(500):\n",
    "    # Run episode till done\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    while not done:\n",
    "        action = pick_sample(state)\n",
    "        state_next, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        memory.add([state, action, reward, state_next, float(done)])\n",
    "        cum_reward += reward\n",
    "\n",
    "        # Train (optimize parameters)\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, n_states, dones = memory.sample(batch_size)\n",
    "            optimize(states, actions, rewards, n_states, dones)\n",
    "            update_target()\n",
    "        s = state_next\n",
    "\n",
    "    # Output total rewards in episode (max 500)\n",
    "    print(\"Run episode{} with rewards {}\".format(i, cum_reward), end=\"\\n\")\n",
    "    reward_records.append(cum_reward)\n",
    "\n",
    "    # stop if reward mean > 475.0\n",
    "    if np.average(reward_records[-10:]) > 2000:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "plt.plot(reward_records)\n",
    "plt.plot(average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down there, there is boilerplate for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    This is going to represent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        batch_size: int,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.dqn = DQN().to(device)\n",
    "        self.target_dqn = DQN().to(device)\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.dqn.parameters(), lr=learning_rate, amsgrad=True\n",
    "        )\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return torch.tensor(\n",
    "                [env.action_space.sample()], device=device, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.dqn(state)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Updates the model\"\"\"\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        )\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.dqn(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = (\n",
    "                self.target_dqn(non_final_next_states).max(1).values\n",
    "            )\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (\n",
    "            next_state_values * self.discount_factor\n",
    "        ) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.dqn.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # save for plotting later on\n",
    "        self.training_error.append(loss.item())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "N_EPISODES = 100\n",
    "START_EPSILON = 1.0\n",
    "EPSILON_DECAY = START_EPSILON / (N_EPISODES / 2)  # reduce the exploration over time\n",
    "FINAL_EPSILON = 0.1\n",
    "\n",
    "UPDATE_TARGET_EVERY = 10\n",
    "\n",
    "agent = Agent(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    initial_epsilon=START_EPSILON,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    final_epsilon=FINAL_EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i_episode in tqdm(range(N_EPISODES)):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # fancy itertools for counting\n",
    "    for t in count():\n",
    "        action = agent.get_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.squeeze(0).cpu().numpy())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        agent.optimize_model()\n",
    "\n",
    "        if i_episode % UPDATE_TARGET_EVERY == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        if terminated or truncated:\n",
    "            agent.decay_epsilon()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = N_EPISODES // 100\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
